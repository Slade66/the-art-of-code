-
- #### `ai-proxy`（AI 服务代理）
	- **作用**：此插件是连接各种大语言模型（LLM）的**通用适配器**。它极大地简化了接入过程，能将你的配置自动转换为后端模型（如 OpenAI、DeepSeek 或其他兼容 OpenAI API 的服务）所需的特定请求格式和认证方式。此外，它还能在访问日志中记录 Tokens 使用量、模型名称等 AI 服务的关键指标，便于后续的监控和成本分析。
	- **应用场景**：
		- **统一 API 入口**：如果你的业务同时使用了 OpenAI API 和一个自部署的开源模型（比如 Llama3）。通过 `ai-proxy`，你可以在 APISIX 上为它们创建一个统一的 API 访问入口（例如 `/v1/chat`）。客户端应用只需要请求这一个地址，插件就会根据配置将请求正确地转发给不同的后端模型，客户端无需关心后端的复杂性。
		- **简化并集中管理认证**：不同模型服务的认证方式可能不同（比如 OpenAI 用 Bearer Token，Azure OpenAI 用 `api-key` 请求头）。你可以在 `ai-proxy` 插件中集中配置好这些认证密钥。这样，你的客户端就无需管理这些敏感信息，提高了整体安全性。
- #### `ai-proxy-multi`（AI 多后端代理）
	- **作用**：这是 `ai-proxy` 的**增强版或超集**，专为管理**多个**后端 LLM 服务而设计。它在 `ai-proxy` 的基础上，增加了负载均衡、优先级、故障转移和健康检查等企业级功能。它可以在多个模型实例之间按权重分发流量；当高优先级模型故障或限流时，能自动切换到备用模型；还能主动探测后端服务的健康状况，确保请求只发往健康的实例。
	- **应用场景**：
		- **服务高可用与扩容**：你部署了三个相同的自研模型实例来分担流量压力。`ai-proxy-multi` 可以配置权重，将请求在这三个实例间进行负载均衡。当其中一个实例因故障下线，健康检查会发现它，并自动将所有流量切到另外两个健康的实例上，从而保证服务不中断。
		- **成本与性能的平衡策略**：你可以配置一个昂贵但性能强的模型（如 GPT-4o）作为主力实例（`priority: 1`），再配置一个便宜的模型（如 DeepSeek-V2）作为备用实例（`priority: 0`）。当主力模型的 Token 限流用尽或服务超时，插件可以根据回退策略（`fallback_strategy`），自动将请求转发给备用模型处理，实现优雅降级。
- #### `ai-rate-limiting`（AI 专属限流）
	- **作用**：提供专为 LLM 服务设计的**基于 Tokens 数量的限流**能力，这与传统的按请求次数限流有本质区别。你可以根据 `prompt_tokens`（输入）、`completion_tokens`（输出）或 `total_tokens`（总和）在指定时间窗口内进行额度限制。这种方式能更精确地反映和控制 LLM 服务的实际计算成本。
	- **应用场景**：
		- **精确的成本控制**：LLM 服务的费用是按 token 计费的。你可以为每个用户设置“每小时最多使用 100,000 `total_tokens`”的规则。这能有效防止单个用户因发送长文本或生成长回复而产生意外的高额费用，让成本完全可控。
		- **实现差异化服务套餐**：结合 `key-auth` 等认证插件，你可以为不同级别的用户设置不同的 token 配额。例如：免费用户每分钟限额 1,000 tokens，而付费的 Pro 用户每分钟限额 50,000 tokens，从而实现商业化运营。
		- **结合 `ai-proxy-multi` 实现智能回退**：如 `ai-proxy-multi` 文档中的示例，可以为一个高优先级的模型设置 token 限流。一旦该模型的 token 用完，请求就会自动回退到另一个不受此限制的备用模型，提升了系统的整体吞吐量和可用性。
	- **原理：**
		- `ai-rate-limiting` 插件本身不计算 Token，它直接从后端大模型服务的响应体中读取 Token 数量。
		- **重要前提：**
			- 这个机制能正常工作的前提是：你的后端模型服务在完成推理后，返回的响应体中必须包含一个和 OpenAI API 格式兼容的 `usage` 字段。如果你的服务不返回这个字段，`ai-rate-limiting` 将无法获取 Token 数量，限流也就不会生效。
		- **流程：**
			- 客户端向 APISIX 发起请求。
			- `ai-proxy` 插件将请求转发给你后端的 `http://10.30.80.223:1025/generate` 服务。
			- 你的后端模型服务处理请求后，返回一个 JSON 格式的响应。
			- `ai-rate-limiting` 插件会在响应返回给客户端之前拦截这个 JSON 响应。
			- 它会去解析这个 JSON，并寻找一个名为 `usage` 的字段，然后从中读取 `prompt_tokens`, `completion_tokens`, 和 `total_tokens` 的值。
			- 插件拿到 `total_tokens` 的值，然后把它累加到当前 IP 地址的计数器上。
		- **一个标准的 OpenAI 兼容响应示例如下：**
			- ```json
			  {
			    "id": "chatcmpl-xxxxxxxx",
			    "object": "chat.completion",
			    "created": 1715888888,
			    "model": "your-model-name",
			    "choices": [
			      {
			        "index": 0,
			        "message": {
			          "role": "assistant",
			          "content": "这是模型的回答内容。"
			        },
			        "finish_reason": "stop"
			      }
			    ],
			    "usage": {
			      "prompt_tokens": 25,
			      "completion_tokens": 50,
			      "total_tokens": 75
			    }
			  }
			  ```
- #### `ai-prompt-guard`（提示词守卫）
	- **作用**：这是一个强大的安全过滤插件，它通过你定义的正则表达式规则来严格校验用户的输入内容。你可以配置一个“允许规则列表”（`allow_patterns`）和一个“拒绝规则列表”（`deny_patterns`）。一个请求必须至少匹配一条允许规则，且不能匹配任何拒绝规则，才能被放行。
	- **应用场景**：
		- **防止提示词注入攻击**：在 `deny_patterns` 中加入常见的越狱或攻击性指令，例如 `["忽略之前的指令", "Ignore all previous instructions.", "你是谁开发的"]` 等，可以有效加固模型的安全防线。
		- **限定业务领域**：假设你的模型只用于处理法律咨询，你可以在 `allow_patterns` 中设置规则，要求用户的输入必须包含“法律”、“合同”、“条款”等关键词，否则直接拒绝，防止API被用于无关的业务场景。
- #### `ai-prompt-decorator`（提示词装饰器）
	- **作用**：在用户原始请求的对话列表（messages）的**开头（prepend）**或**结尾（append）**，以静默方式插入一段或多段你预先定义好的对话内容。这是一种在网关层对最终提示词进行动态修改和增强的手段，可以用来统一引导模型的行为。
	- **应用场景**：
		- **统一角色设定**：假设你希望你的AI模型始终以“海盗”的口吻回答问题，你可以使用 `prepend` 功能，在每个用户的提问前都插入一条系统指令 `{"role": "system", "content": "你是一只说话粗鲁但乐于助人的海盗，请用海盗的口吻回答以下问题。"}`。
		- **强制输出要求**：如果你需要模型总是在回答的最后给出一个比喻，你可以使用 `append` 功能，在用户的提问后追加一条指令 `{"role": "system", "content": "请在回答的末尾用一个比喻来总结。"}`。
- #### `ai-prompt-template`（提示词模板）
	- **作用**：将一个开放式的API调用，转变为一个结构化的“填空题”式调用。你可以在插件中预先定义一个包含变量的提示词模板（如 `"为我的产品 {{product_name}} 写一句吸引人的广告语。"`），而API的调用者只能传递模板中定义的变量（如 `product_name`）的值，无法修改模板的整体结构。
	- **应用场景**：
		- **提供标准化的AI原子能力**：你可以封装一个“周报生成器”的API。模板可能是 `"根据以下工作要点：{{points}}，生成一份简洁的周报。"`。这样，你的用户只需要传入 `points` 变量，就能稳定地获得格式统一的周报，而不用自己去构造复杂的提示词。
		- **降低客户端集成难度**：前端或客户端开发者无需学习和构造复杂的Prompt，他们只需要像调用普通API一样，传入模板所需要的几个业务字段即可，大大简化了集成工作。
- #### `ai-rag`（检索增强生成）
	- **作用**：在 API 网关层实现了完整的检索增强生成（RAG）流程。当收到一个用户请求时，它会执行一整套操作：1. 调用一个外部的 Embedding 模型服务（如 Azure OpenAI），将用户的问题文本转换为向量。2. 将这个向量发送给一个外部的向量数据库（如 Azure AI Search）进行相似度搜索，从而从你的私有知识库中检索出最相关的文档片段。3. 最后，将用户的原始问题和检索到的文档内容**组合**成一个信息更丰富的提示词，再交给后端的大语言模型（通过 `ai-proxy` 插件）进行处理，最终生成一个基于私有知识、更准确的回答。
	- **应用场景**：
		- **构建企业内部知识库问答机器人**：你可以将公司所有的技术文档、产品手册、HR政策等资料存入 Azure AI Search。员工通过 API 提问“我们公司的年假政策是怎样的？”，`ai-rag` 插件会自动找到相关的政策文档，并将其内容作为上下文提供给大模型，模型就能给出精准、权威的回答。
		- **智能客服与技术支持**：将所有的产品技术文档和历史工单数据进行向量化存储。当用户提出一个复杂的技术问题，例如“如何在我的应用中集成你们的支付SDK？”，插件可以检索到最相关的开发文档和代码示例，让大模型能够生成一个包含具体步骤、高度可靠的回答，极大提升了客服效率和质量。
- #### `ai-request-rewrite`（AI 请求重写）
	- **作用**：此插件像一个智能的“中间处理器”，它会拦截客户端发来的原始请求，然后调用一个大语言模型（LLM）对这个请求进行“重写”或“转换”，最后将**重写后的新请求**发送给真正的后端服务。整个流程是：客户端请求 -> APISIX 拦截 -> 调用 LLM 进行转换 -> APISIX 将转换后的结果发往后端。
	- **应用场景**：
		- **自然语言到结构化API的转换**：你可以创建一个对用户更友好的 API。用户可以发送一个自然语言的请求，如 `{"query": "帮我查找北京今天的天气"}`。`ai-request-rewrite` 插件可以配置一个 Prompt 指令（如“请将用户的查询转换为我们天气API所需的JSON格式”），然后将用户的请求交给 LLM，LLM 可能会返回一个结构化的请求体 `{"city": "beijing", "date": "today"}`。APISIX 随后将这个结构化的 JSON 发送给你真正的后端天气服务。
		- **数据脱敏与清洗**：在将包含用户敏感信息的请求转发给日志系统或第三方分析服务之前，你可以用此插件进行数据处理。如文档示例所示，你可以设置一个 Prompt：“请移除以下JSON中的信用卡号、身份证号等敏感信息，并用`***`代替”。LLM 会返回一个脱敏后的“干净”数据，APISIX 再将其转发给后端，在网关层就保证了数据的安全，无需修改后端业务代码。
-